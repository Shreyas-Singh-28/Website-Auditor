{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install -q google-generativeai beautifulsoup4 requests gradio"
      ],
      "metadata": {
        "id": "VNUYy8l5Mo3m"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import ssl\n",
        "import socket\n",
        "import json\n",
        "import gradio as gr\n",
        "from google.colab import userdata\n",
        "from google import genai\n",
        "\n",
        "API_KEY = userdata.get('GEMINI_API_KEY')\n",
        "client = genai.Client(api_key=API_KEY)\n",
        "\n",
        "def seo_checks(url):\n",
        "    results = {\n",
        "        \"technical_seo\": {},\n",
        "        \"on_page_seo\": {},\n",
        "        \"security\": {},\n",
        "        \"mobile_usability\": {},\n",
        "        \"performance\": {},\n",
        "        \"user_experience\": {},\n",
        "        \"analytics_tracking\": {}\n",
        "    }\n",
        "\n",
        "    try:\n",
        "        r = requests.get(url, timeout=10)\n",
        "        soup = BeautifulSoup(r.text, \"html.parser\")\n",
        "        results[\"technical_seo\"][\"status_code\"] = r.status_code\n",
        "\n",
        "        results[\"on_page_seo\"][\"title\"] = soup.title.string if soup.title else \"Missing\"\n",
        "        meta_desc = soup.find(\"meta\", attrs={\"name\": \"description\"})\n",
        "        results[\"on_page_seo\"][\"meta_description\"] = meta_desc[\"content\"] if meta_desc and \"content\" in meta_desc.attrs else \"Missing\"\n",
        "\n",
        "        results[\"security\"][\"https\"] = url.startswith(\"https\")\n",
        "\n",
        "        images = soup.find_all(\"img\")\n",
        "        missing_alt = sum(1 for img in images if not img.get(\"alt\"))\n",
        "        results[\"on_page_seo\"][\"missing_image_alts\"] = missing_alt\n",
        "\n",
        "        internal_links = [a.get(\"href\") for a in soup.find_all(\"a\", href=True) if url in a.get(\"href\")]\n",
        "        results[\"on_page_seo\"][\"internal_links\"] = len(internal_links)\n",
        "\n",
        "        results[\"analytics_tracking\"][\"google_analytics\"] = \"UA-\" in r.text or \"gtag(\" in r.text\n",
        "\n",
        "    except Exception as e:\n",
        "        results[\"error\"] = str(e)\n",
        "\n",
        "    return results\n",
        "\n",
        "def audit_with_llm(url):\n",
        "    if not url.strip():\n",
        "        yield \"⚠️ Please enter a valid URL\"\n",
        "        return\n",
        "\n",
        "    yield \"⏳ Processing...\"\n",
        "    python_results = seo_checks(url)\n",
        "\n",
        "    prompt = f\"\"\"\n",
        "    You are an SEO expert. Based on the JSON data below, evaluate the website in these categories and return a well-structured markdown report with headings and bullet points.\n",
        "\n",
        "    ### Categories to Evaluate:\n",
        "    1. Technical SEO & Crawlability\n",
        "    2. On-Page SEO\n",
        "    3. Security\n",
        "    4. Mobile Usability and Responsiveness\n",
        "    5. Performance & Core Web Vitals\n",
        "    6. User Experience (UX/UI)\n",
        "    7. Analytics & Tracking\n",
        "    For each category:\n",
        "        - Give a **score out of 10**\n",
        "        - List **observations**\n",
        "    For final website score:\n",
        "        - Give a **score out of 70**\n",
        "        - Give **recommendations**\n",
        "    JSON Data:\n",
        "    {json.dumps(python_results, indent=2)}\n",
        "    \"\"\"\n",
        "\n",
        "    try:\n",
        "        resp = client.models.generate_content(model=\"gemini-2.5-flash\", contents=prompt)\n",
        "        gemini_output = getattr(resp, \"text\", str(resp))\n",
        "    except Exception as e:\n",
        "        gemini_output = f\"Gemini error: {e}\"\n",
        "\n",
        "    yield gemini_output\n",
        "\n",
        "with gr.Blocks() as demo:\n",
        "    with gr.Column(elem_classes=\"centered\"):\n",
        "        gr.Markdown(\"# Simple Website Auditor\", elem_classes=\"centered\")\n",
        "        url_input = gr.Textbox(label=\"Website URL\", placeholder=\"https://example.com\", elem_classes=\"centered\")\n",
        "        with gr.Row(elem_classes=\"centered\"):\n",
        "            submit_btn = gr.Button(\"Submit\", elem_classes=\"centered\")\n",
        "            clear_btn = gr.Button(\"Clear\", elem_classes=\"centered\")\n",
        "        output_md = gr.Markdown()\n",
        "\n",
        "        submit_btn.click(fn=audit_with_llm, inputs=url_input, outputs=output_md)\n",
        "        clear_btn.click(lambda: (\"\", \"\"), None, [url_input, output_md])\n",
        "\n",
        "demo.launch()\n"
      ],
      "metadata": {
        "id": "Rk-57hXjrElA",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 650
        },
        "outputId": "44c5b20b-323c-4721-8726-002406ea131a"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "It looks like you are running Gradio on a hosted Jupyter notebook, which requires `share=True`. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://84570ee6b04ea5d187.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://84570ee6b04ea5d187.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    }
  ]
}